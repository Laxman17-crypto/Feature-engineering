{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJrb2QOcJSLg"
   },
   "source": [
    "#Assignmnent Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVXAibFqJSSd"
   },
   "source": [
    "**1.What is parameter?**\n",
    "\n",
    "A parameter in the context of feature engineering, especially in libraries like PyCaret , can refer to settings or arguments used within functions or methods that control how feature engineering is performed.\n",
    "\n",
    "For example, in PyCaret's setup function, there's a group_features parameter . This parameter is used to specify groups of related features from which new statistical features (like mean, median, variance, and standard deviation) can be generated ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6q_GMa0JSYn"
   },
   "source": [
    "**2.What is correlation?\n",
    "What does negative correlation mean?**\n",
    "\n",
    "Correlation is a statistical measure that describes the extent to which two variables change together. It tells you how closely two variables are related and how they move in relation to one another.\n",
    "\n",
    "Negative correlation means that as one variable increases, the other variable tends to decrease. They move in opposite directions. For example, there might be a negative correlation between the number of hours a student watches TV and their exam scores; as TV time increases, exam scores might decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEzGpq25JScd"
   },
   "source": [
    "**3.Define Machine Learning. What are the main components in Machine Learning?**\n",
    "\n",
    "Machine Learning is a subset of artificial intelligence that enables systems to learn from data and make predictions or decisions without being explicitly programmed. Instead of following rigid, predefined rules, machine learning algorithms identify patterns and relationships within data to improve their performance over time.\n",
    "\n",
    "The main components of Machine Learning are:\n",
    "\n",
    "1.  Data: This is the raw material for machine learning. The quality, quantity, and relevance of the data significantly impact the performance of a machine learning model. Data can come in various forms, such as numerical, categorical, text, or images.\n",
    "2.  Algorithm: This is the set of instructions or mathematical procedures that the machine learning system uses to learn from the data. Different algorithms are suited for different types of problems and data structures. Examples include linear regression, decision trees, support vector machines, and neural networks.\n",
    "3.  Model: The model is the output of the learning process. It represents the learned patterns and relationships from the data, encoded in the algorithm. The model is then used to make predictions or decisions on new, unseen data.\n",
    "4.  Evaluation: This involves assessing the performance of the trained model. Various metrics are used to measure how well the model generalizes to new data, such as accuracy, precision, recall, and F1-score.\n",
    "5.  Optimization: This is the process of tuning the parameters of the algorithm to improve the model's performance based on the evaluation results. Optimization techniques aim to minimize errors and maximize the model's ability to make accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Glpd40P7d1ig"
   },
   "source": [
    "**4.How does loss value help in determining whether the model is good or not?**\n",
    "\n",
    "The loss value is a crucial indicator of a machine learning model's performance during training. Here's how it helps determine if a model is good:\n",
    "\n",
    "1. Quantifying Error: A loss function mathematically measures the difference between the model's predicted output and the actual values. [1] A lower loss value indicates that the model's predictions are closer to the actual values, meaning it's making fewer errors. Conversely, a higher loss value suggests larger discrepancies and a less accurate model.\n",
    "\n",
    "2. Guiding Optimization: During the training process, the model's parameters are adjusted to minimize the loss function. [1] The loss value acts as a guide for this optimization. If the loss is high, the optimization algorithm makes larger adjustments to the parameters to try and reduce the error. As the loss decreases, the adjustments become smaller, and the model converges towards a better set of parameters.\n",
    "\n",
    "3. Monitoring Training Progress: By tracking the loss value over training epochs (iterations), you can see how well the model is learning. A decreasing loss curve generally indicates that the model is improving. If the loss plateaus or starts increasing, it might suggest issues like overfitting or an insufficient learning rate.\n",
    "\n",
    "4. Comparison Between Models: The loss value can be used to compare the performance of different models or different variations of the same model. A model with a consistently lower loss on a validation dataset (data the model hasn't seen during training) is generally considered better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajNopprueOGx"
   },
   "source": [
    "**5.What are continuous and categorical variables?**\n",
    "\n",
    "Categorical Variables:\n",
    "\n",
    "-  Represent distinct categories or groups.\n",
    "-  Examples: Gender (Male, Female), color (Red, Blue, Green), types of animals (Dog, Cat, Bird).\n",
    "-  Can be nominal (no inherent order, like colors) or ordinal (have a meaningful order, like t-shirt sizes: S, M, L).\n",
    "\n",
    "Continuous Variables:\n",
    "\n",
    "-  Can take on any value within a given range.\n",
    "-  Examples: Height (1.75m, 1.80m), temperature (25.5°C, 30°C), time (measured in seconds, minutes, etc.).\n",
    "-  Can be interval (equal intervals represent equal differences, but no true zero, like temperature in Celsius) or ratio (equal intervals and a true zero point, like height)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nsHQogaUefy4"
   },
   "source": [
    "**6.How do we handle categorical variables in Machine Learning? What are the common techniques?**\n",
    "\n",
    "In Machine Learning, we handle categorical variables by converting them into a numerical format that algorithms can understand. Here are some common techniques:\n",
    "\n",
    "1.  One-Hot Encoding: This technique creates a new binary column for each category in the variable. If a data point belongs to a category, the corresponding column will have a value of 1, and all other category columns will have a value of 0. This is a widely used method, especially for nominal categorical variables where there's no inherent order. [1]\n",
    "\n",
    "2.  Label Encoding: This method assigns a unique integer to each category. For example, if a variable has categories \"Red\", \"Blue\", and \"Green\", they might be encoded as 0, 1, and 2 respectively. This is suitable for ordinal categorical variables where the order of categories matters.\n",
    "\n",
    "3.  Binary Encoding: This technique is a combination of Label Encoding and One-Hot Encoding. It first converts the categories into numerical form using Label Encoding and then represents these integers in binary code. Each bit of the binary code becomes a new column.\n",
    "\n",
    "4. Frequency Encoding: This method replaces each category with its frequency (or count) in the dataset.\n",
    "\n",
    "5. Ordinal Encoding: Similar to Label Encoding, but it specifically considers the order of the categories if there is one. You explicitly define the order of the categories and assign increasing integers based on that order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iaHLDgIe1k5"
   },
   "source": [
    "**7.What do you mean by training and testing a dataset?**\n",
    "\n",
    "Training the Dataset:\n",
    "\n",
    "- Purpose: The goal of training is to allow the machine learning model to learn patterns, relationships, and structures within the data. Think of it as the model going to school and studying.\n",
    "\n",
    "\n",
    "Testing the Dataset:\n",
    "\n",
    "- Purpose: The purpose of testing is to evaluate how well the trained model performs on data it has never seen before. This is crucial for assessing the model's ability to generalize to new data and avoid overfitting (where the model performs well on the training data but poorly on new data).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvF-L04ofLAg"
   },
   "source": [
    "**8.What is sklearn.preprocessing?**\n",
    "\n",
    "sklearn.preprocessing is a module in the scikit-learn library that provides a collection of tools for preprocessing data. Preprocessing is a crucial step in machine learning as it transforms raw data into a format that is more suitable for machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0LjhykafZzo"
   },
   "source": [
    "**9.What is a Test set?**\n",
    "\n",
    "a test set is a portion of your dataset that is held out and not used during the training process of a machine learning model.It is used to to:\n",
    "-  Evaluating Generalization: The primary purpose of a test set is to evaluate how well your trained model performs on data it has never seen before.\n",
    "-  Unbiased Performance Assessment: By testing on a separate, unseen dataset, you get an unbiased estimate of your model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87nD24lkgVxL"
   },
   "source": [
    "**10.How do we split data for model fitting (training and testing) in Python?\n",
    "How do you approach a Machine Learning problem?**\n",
    "\n",
    "In Python, data is typically split for model fitting (training and testing) using the train_test_split function from scikit-learn. A general approach to a machine learning problem involves data collection, preparation, model selection, training, evaluation, hyperparameter tuning, and prediction.\n",
    "Data Splitting:\n",
    "-  Import train_test_split: Import the necessary function from the scikit-learn library: from sklearn.model_selection import train_test_split.\n",
    "-  Prepare your data: Ensure your data is in a suitable format (e.g., Pandas DataFrame) and that you have clearly separated features (independent variables) and the target (dependent variable).\n",
    "-  Call train_test_split: Use the function to split your data into training and testing sets:\n",
    "Python\n",
    "\n",
    "\n",
    "Machine Learning Problem Approach:\n",
    "- Data Collection: Gather the relevant data, ensuring it's representative of the problem you're trying to solve.\n",
    "- Data Preparation: Clean, preprocess, and transform the data to make it suitable for model training. This might involve handling missing values, encoding categorical variables, and scaling numerical features.\n",
    "- Model Selection: Choose an appropriate machine learning algorithm based on the problem type (e.g., regression, classification) and the characteristics of your data.\n",
    "- Model Training: Train your chosen model on the training data.\n",
    "- Model Evaluation: Evaluate the model's performance on the test data using appropriate metrics (e.g., accuracy, precision, recall, F1-score for classification; R-squared, mean squared error for regression).\n",
    "- Hyperparameter Tuning: Optimize the model's parameters to improve its performance. Techniques like grid search or random search can be used to find the best combination of hyperparameters.\n",
    "- Prediction: Use the trained model to make predictions on new, unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwW-204phwZg"
   },
   "source": [
    "**11.Why do we have to perform EDA before fitting a model to the data?**\n",
    "\n",
    "Before fitting a machine learning model, it's crucial to perform Exploratory Data Analysis (EDA). EDA is the process of examining and visualizing data to understand its characteristics. Here's why it's important:\n",
    "\n",
    "- Understanding the Data: EDA helps you gain insights into the structure, relationships, and patterns within your dataset. You can identify the types of variables, their distributions, and potential correlations.\n",
    "\n",
    "- Identifying Data Quality Issues: EDA allows you to detect issues like missing values, outliers, and inconsistencies in the data. Addressing these issues before modeling can significantly improve the model's performance.\n",
    "\n",
    "- Feature Selection and Engineering: By understanding the data, you can identify relevant features for your model and potentially create new, more informative features.\n",
    "\n",
    "- Choosing the Right Model: EDA can provide clues about which types of machine learning algorithms might be suitable for your problem based on the data characteristics and relationships observed.\n",
    "\n",
    "- Detecting Bias: EDA can help uncover potential biases in the data that might affect the model's fairness and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WjGI_FHh7bp"
   },
   "source": [
    "**12.What is correlation?**\n",
    "\n",
    "correlation is a statistical measure that describes the extent to which two variables change together. It indicates how closely two variables are related and how they move in relation to one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxBz4ELpisII"
   },
   "source": [
    "**13.What does negative correlation mean?**\n",
    "\n",
    "negative correlation means that as one variable increases, the other variable tends to decrease. They move in opposite directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUhtme2cjKKJ"
   },
   "source": [
    "**14.How can you find correlation between variables in Python?**\n",
    "\n",
    "Using Pandas:\n",
    "\n",
    "If your data is in a Pandas DataFrame, you can use the .corr() method to calculate the pairwise correlation between columns.\n",
    "\n",
    "Using NumPy:\n",
    "\n",
    "You can also use NumPy to calculate the correlation coefficient between two arrays using the numpy.corrcoef() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iMnFy0wCjxdL"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a Pandas DataFrame named 'df'\n",
    "# Create a sample DataFrame\n",
    "data = {'col1': [1, 2, 3, 4, 5],\n",
    "        'col2': [5, 4, 3, 2, 1],\n",
    "        'col3': [1, 2, 5, 4, 3]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XwWgI5SKjyjS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create two NumPy arrays\n",
    "array1 = np.array([1, 2, 3, 4, 5])\n",
    "array2 = np.array([5, 4, 3, 2, 1])\n",
    "\n",
    "# Calculate the correlation coefficient between the two arrays\n",
    "correlation_coefficient = np.corrcoef(array1, array2)[0, 1]\n",
    "\n",
    "# Display the correlation coefficient\n",
    "print(correlation_coefficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-QJb20vjqXJ"
   },
   "source": [
    "**15.What is causation? Explain difference between correlation and causation with an example.**\n",
    "\n",
    "Causation means that one event is the direct result of another event. In other words, a change in one variable causes a change in another variable.\n",
    "\n",
    "The key difference between correlation and causation is that correlation shows an association or relationship between two variables, while causation indicates that one variable directly influences or brings about a change in the other. Correlation does not imply causation. Just because two variables move together doesn't mean one causes the other; there might be a third, unseen factor influencing both.\n",
    "\n",
    "Example:\n",
    "\n",
    "- Correlation: You might observe a positive correlation between ice cream sales and the number of drownings. As ice cream sales increase, so does the number of drownings.\n",
    "- Causation: Does eating ice cream cause people to drown? No. The correlation exists because both ice cream sales and drownings tend to increase during the summer months due to warmer weather. The warm weather is the confounding factor causing both to rise. Therefore, there is a correlation but no causation between ice cream sales and drownings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pu30RsJJkM_x"
   },
   "source": [
    "**16.What is an Optimizer? What are different types of optimizers? Explain each with an example.**\n",
    "\n",
    "An optimizer is an algorithm or method used to adjust the parameters of a machine learning model during the training process to minimize the loss function . In essence, optimizers help the model learn by finding the best set of weights and biases that result in the lowest error between the model's predictions and the actual target values.\n",
    "\n",
    "Different types of optimizers exist, each with its own approach to updating model parameters:\n",
    "\n",
    "- SGD (Stochastic Gradient Descent): This is a basic optimizer that updates parameters in the direction of the negative gradient of the loss function. It computes the gradient and updates parameters for each training example one at a time [1]. This can be noisy but can escape local minima.\n",
    "\n",
    "\n",
    "\n",
    "- Adam: This is a popular adaptive learning rate optimization algorithm. It combines the ideas of RMSprop and momentum, calculating individual adaptive learning rates for different parameters [1]. It generally performs well in practice.\n",
    "\n",
    "\n",
    "- RMSprop: This optimizer divides the learning rate by an exponentially decaying average of squared gradients [1]. This helps to accelerate convergence in the direction of shallow gradients and slow down in the direction of steep gradients.\n",
    "\n",
    "\n",
    "\n",
    "Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tAbKE72gll76"
   },
   "outputs": [],
   "source": [
    "#1.SGD\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "    optimizer = SGD(learning_rate=0.01)\n",
    "\n",
    "#2.Adam\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "#3.RMSprop\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "    optimizer = RMSprop(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vzjEiipl46h"
   },
   "source": [
    "**17.What is sklearn.linear_model ?**\n",
    "\n",
    "sklearn.linear_model is a module within the scikit-learn library that provides various linear models for regression and classification tasks. [1] These models aim to find a linear relationship between the input features and the target variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKiASLaQ5wnW"
   },
   "source": [
    "**18.What does model.fit() do? What arguments must be given?**\n",
    "\n",
    "In the context of machine learning libraries like scikit-learn, model.fit() is a method used to train a machine learning model.\n",
    "\n",
    "- Learning from Data: The fit() method takes your training data (features and target variable) and allows the model to learn the underlying patterns, relationships, and parameters that best describe the data. It essentially adjusts the internal parameters of the model based on the provided training data to minimize a defined loss function (for supervised learning).\n",
    "\n",
    "- Building the Model: After the fit() method completes, the model has been \"trained\" and is ready to make predictions on new, unseen data.\n",
    "\n",
    "The arguments that must be given to model.fit() typically include:\n",
    "\n",
    "- X: This represents the training data's features (input variables). It should be in a format that the model can process, commonly a NumPy array or a Pandas DataFrame. Each row represents a data instance, and each column represents a feature.\n",
    "- y: This represents the target variable (output variable) for the training data. It should correspond to the X data, with each value being the expected output for the corresponding row in X. It's usually a NumPy array or a Pandas Series.\n",
    "\n",
    "Some models might accept additional optional arguments in fit(), such as:\n",
    "\n",
    "- sample_weight: For assigning different weights to individual training samples.\n",
    "- validation_data: For providing a separate dataset to evaluate the model's performance during training (useful for monitoring overfitting).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Hq2eYc36dl3"
   },
   "source": [
    "**19.What does model.predict() do? What arguments must be given?**\n",
    "\n",
    "In machine learning libraries like Keras or scikit-learn, model.predict() is a method used to generate predictions from a trained model.\n",
    "\n",
    "Here's what model.predict() does:\n",
    "\n",
    "- Generates Output: Once you have trained your model using the model.fit() method, you use model.predict() to get the model's output for new, unseen input data.\n",
    "Applies Learned Patterns: The method applies the patterns and relationships learned by the model during training to the new input data to produce predictions.\n",
    "\n",
    "The arguments that must be given to model.predict() typically include:\n",
    "\n",
    "- X: This represents the input data (features) for which you want to make predictions. It should be in the same format that the model was trained on, commonly a NumPy array or a Pandas DataFrame. The structure (number of features/columns) of this input data must match the input shape of the model. [2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDrkxcWB6r92"
   },
   "source": [
    "**20.What are continuous and categorical variables?**\n",
    "\n",
    "Categorical Variables:\n",
    "\n",
    "- Represent distinct categories or groups.\n",
    "- Examples provided: Gender (Male, Female), color (Red, Blue, Green), types of animals (Dog, Cat, Bird).\n",
    "\n",
    "Continuous Variables:\n",
    "\n",
    "- Can take on any value within a given range.\n",
    "- Examples provided: Height (1.75m, 1.80m), temperature (25.5°C, 30°C), time (measured in seconds, minutes, etc.).\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kfQlMAz6-gn"
   },
   "source": [
    "**21.What is feature scaling? How does it help in Machine Learning?**\n",
    "\n",
    "Feature scaling is a data preprocessing technique used to standardize the range of independent variables or features in a dataset. It involves transforming the values of features to a similar scale, often within a specific range (like 0 to 1 or -1 to 1) or with a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "How it helps in Machine Learning:\n",
    "\n",
    "- Improved Model Performance: Many machine learning algorithms are sensitive to the scale of the input features. Algorithms that use distance metrics (like K-Nearest Neighbors, Support Vector Machines, and K-Means Clustering) or gradient descent (like linear regression, logistic regression, and neural networks) can be significantly affected by features with different scales. Features with larger values might dominate the learning process, leading to biased or suboptimal models. Scaling ensures that all features contribute more equally.\n",
    "- Faster Convergence: For optimization algorithms like gradient descent, feature scaling can lead to faster convergence to the optimal solution. When features are on different scales, the cost function's contours can be elongated and narrow, making it difficult for the optimizer to find the minimum efficiently. Scaling makes the contours more circular, allowing for a more direct path to the minimum.\n",
    "- Preventing Dominance of Features: Without scaling, features with larger numerical ranges can disproportionately influence the model's decision-making process, even if they are not the most important features. Scaling prevents this by ensuring that all features have a comparable impact.\n",
    "- Regularization Effectiveness: Regularization techniques (like L1 and L2 regularization) that penalize large coefficients can be more effective when features are scaled. Without scaling, coefficients for features with larger values might be unfairly penalized less than those for features with smaller values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSQixGSd7RLm"
   },
   "source": [
    "**22.How do we perform scaling in Python?**\n",
    "\n",
    "You can perform feature scaling in Python using the sklearn.preprocessing module from the scikit-learn library. This module provides several scaling techniques. Here are a couple of common ones with examples:\n",
    "\n",
    "1. StandardScaler\n",
    "\n",
    "StandardScaler standardizes features by removing the mean and scaling to unit variance. The formula for standardization is:\n",
    "\n",
    "$z = (x - \\mu) / \\sigma$\n",
    "\n",
    "where $\\mu$$\\mu$ is the mean and $\\sigma$$\\sigma$ is the standard deviation.\n",
    "\n",
    "2. MinMaxScaler\n",
    "\n",
    "MinMaxScaler scales features to a given range, usually between 0 and 1. The formula for min-max scaling is:\n",
    "\n",
    "$X_{scaled} = (X - X_{min}) / (X_{max} - X_{min})$\n",
    "\n",
    "where $X_{min}$$X_{min}$ is the minimum value of the feature and $X_{max}$$X_{max}$ is the maximum value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VS4z7iw7ioe"
   },
   "source": [
    "**23.What is sklearn.preprocessing?**\n",
    "\n",
    "sklearn.preprocessing is a module in the scikit-learn library that provides a collection of tools for preprocessing data. Preprocessing is a crucial step in machine learning as it transforms raw data into a format that is more suitable for machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8bd280770_2"
   },
   "source": [
    "**24.How do we split data for model fitting (training and testing) in Python?**\n",
    "\n",
    "You can split data for model fitting (training and testing) in Python using the train_test_split function from the sklearn.model_selection module.\n",
    "\n",
    "Here are the general steps:\n",
    "\n",
    "1. Import train_test_split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xzEFDe_D8Bvt"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsaEhtZs8C8W"
   },
   "source": [
    "2. Prepare your data: Ensure your features (independent variables) and target (dependent variable) are separated, typically in NumPy arrays or Pandas DataFrames. Let's say your features are in X and your target is in y.\n",
    "\n",
    "3. Call train_test_split: Use the function to split your data. The essential arguments are your features (X) and target (y). You can also specify the test_size (the proportion of the dataset to include in the test split) and random_state (for reproducibility)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6MqQ-PvJ8Ged"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64OcI8r48Mem"
   },
   "source": [
    "**25.Explain data encoding?**\n",
    "\n",
    "Data encoding is the process of converting categorical variables into numerical representations that machine learning algorithms can understand. Many machine learning models require numerical input, so this is a necessary step when dealing with categorical data. [1]\n",
    "\n",
    "There are various techniques for data encoding, such as:\n",
    "\n",
    "- One-Hot Encoding: Creates binary columns for each category. [1]\n",
    "- Label Encoding: Assigns unique integers to categories.\n",
    "- Ordinal Encoding: Similar to Label Encoding, but considers the order of categories.\n",
    "- Frequency Encoding: Replaces categories with their frequency.\n",
    "\n",
    "The choice of encoding method depends on the nature of the categorical variable (nominal or ordinal) and the specific machine learning algorithm being used."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMHgMOZp1H9stxTTCZiUjtR",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
